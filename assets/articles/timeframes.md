### Is Superintelligence Coming?

notes:
Disclaimer: my opinions are based on relatively modest research as I’m pretty early in the journey. I invite any corrections to anything I’ve written that is wrong.

The current hype cycle presents our incremental improvements and applications of modern machine learning techniques as rapid steps towards a fuzzy and general “AI” concept. There are some cool sounding achievements being announced almost weekly, but they’re all still within extremely narrow specifications. 

Stepping back a bit, in my opinion most of modern machine learning techniques (an example chosen because they’re popular in the media at the moment) are more about smart programmers applying clever statistical techniques than scientists creating a process that might actually result (by itself, or individually significantly contribute to) in emergent consciousness or a generalised intelligence.

As an example, deep neural networks are:
- not very deep. A depth of 30 nodes (a settable hyperparameter chosen by the person creating the network, realistically limited by hardware) is impressively deep (http://thenewstack.io/deep-learning-neural-networks-google-deep-dream/) whereas real neurons equivalent “depth” is between 2 and 10,000-100,000 (https://www.reddit.com/r/MachineLearning/comments/432hht/depth_of_the_human_neural_network/czfbj4n/).
- not really neural. Apart from being loosely inspired by neurons, a node in a neural network is a simple number/transformation compared with the deeply complex neurons (of which there are many different types) in our brains.
- barely networked - the number of local connections each neuron has in something like a convolutional neural network is also a hyperparameter but essentially always minuscule by comparison to their real squishy counterparts (http://www.human-memory.net/brain_neurons.html).

Fundamentally, deep neural nets don’t really model the brain and aren’t really particularly related to concepts of general intelligence (in my opinion) in the same way the brain might be (http://www.turingfinance.com/misconceptions-about-neural-networks/).

My point is that the hype that creates a sense of nearness does not match up with reality. AI research has progressed, like many fields, at a fairly steady pace since it’s inception.

Research is a slow process and we are so incredibly far from understanding the basis of intelligence itself, I think it is hopelessly optimistic to suggest we are anywhere near creating (and controlling) it artificially. (A cool read on that https://www.engadget.com/2016/08/15/technological-singularity-problems-brain-mind/). Ask a room full of neuroscientists and neurologists how intelligence works and you’ll get a bunch of laughter.

This is getting a tad long for a Facebook comment and the arguments are not properly sourced so I’ll write up a proper blog post at some point as this is an interesting topic. I’ll share it here when I get around to it!

Finally, just to be clear, I still think that the possibility of creating general artificial intelligence in 100 years is a terrifying concept and we’re being typically shortsighted as a species and not dealing with it properly at all. Hence my membership of this group and enjoyment of your podcasts!

notes 2:
So, I’m not sure I agree that it’s meaningful to realistically describe progress towards general intelligence as a number.

I come originally from a finance background so I’m familiar with the power of exponential growth. However, for the concept of exponential growth to apply, certain conditions must be met. Growth must occur across the corpus of the subject at a stable rate over time, in order for growth to build on growth.

In finance, this is simple. You have capital k, and interest rate r. Capital is just a number of dollars. If it goes up from 100 to 105, then the next 5% increase will obviously take it to 110.25.

In our case, we’re talking about “growth of knowledge/capability in the field of artificial intelligence”, which is much broader and more complex than a number of dollars.

It’s not wholly clear to me which parts of that growth, or progress, will build on previous knowledge *in a way that is multiplicative with past or future progress*.

For example, if you learn something about intelligence, say that a goal setting mechanism can be influenced in a particular way, does that necessarily mean you will learn the next thing about intelligence faster in your research? Probably not. However some progress, for example increases in computer hardware speeds on which we run our AI software, definitely has increased along an exponential curve.

This actually provides a good example of what I’m talking about though.

Moore’s law predicted that the number of transistors on a circuitboard would double every two years. This law is often generalised in popular media along the lines of “computers get twice as fast every two years” or something similar. Now computers certainly did get much faster for a long time in this sort of way. However, even in this area growth is not simplistic enough to apply the concept of exponentially without some caveats. Due to real world physical limits and the properties of atomic particles, we can’t actually just continue to make things smaller forever. Various clever techniques continued progress through the 2000s for some time, but moore’s law has most definitely died (https://arstechnica.com/information-technology/2016/02/moores-law-really-is-dead-this-time/).

In reality, the concept of there being a “law” that enabled continuous exponential growth, was really just a nice narrative and in fact the concept of exponential growth in technological improvement is a huge oversimplification of the hugely multifarious nature of the growth itself, which is in fact many small unrelated discoveries, inventions, progresses made towards individual problems and so on.

Certainly, improvements in one area help improvements in another. The internet for example has massively increased the rate of growth in many other discovery pursuits. The development of GAI would most certainly do something similar, maybe even much more profoundly. However, we can’t simply generalise this concept to the field of AI research and just expect that all progress towards unlocking all the many capabilities required to actually create a GAI can be represented as a simple number and that all progress builds on itself in a multiplicative fashion and therefore will grow exponentially.

So to summarise another embarrassingly long Facebook comment, I think that it doesn’t necessarily make sense to consider “progress in science towards the ability to build general AI” as something simple enough to apply a linear or exponential growth model to. Progress in the field is much more nuanced, and while definitely increasing with increased resourcing in recent times, there is still a lot of slow, hard research to do in order to solve all the technical problems ahead.
