## Is Super-intelligence Coming?

Disclaimer: my opinions are based on pretty modest research as I’m early in the journey. I invite any corrections or factual counterarguments to the content below!

A common concern raised in discussions about Artificial Intelligence research, particularly by people outside the field, is (paraphrasing a little) “AI sounds cool but we probably shouldn’t make super-intelligent robots that will destroy us all, ending human civilisation forever.” Terrifying.

Luminaries such as Stephen Hawking, Bill Gates and Elon Musk (all notably brilliant, highly effective and not AI researchers) are amongst those who think we might accidentally extinguish our species as a side-effect of creating a super powerful and uncontrollable AI. SOURCE LINKS

On the other hand, those working professionally as AI researchers generally seem not to raise these kinds of concerns and indeed often argue against them. Perhaps they just don’t want to kill off research funding to the area? Then again, if a researcher were faced with a reduction in funding or the destruction of our species, I feel like they probably wouldn’t choose the latter. SOURCE LINKS

So, is super-intelligence coming? Should we be concerned? Why is more concern coming from smart people outside the field than from within?

Note: if you’d like to know what definition of intelligence I’m using, as it can be a fuzzy topic, it’s discussed here: LINK

The first question is possibly more straightforward than it seems at first blush. I contend that if you accept that intelligence arises as a phenomenon resultant from the properties of tangible physical materials merely arranged in specific ways, then it seems like that we will unlock the secrets of intelligence at some point. The continued application of effort and the scientific method should get us there eventually.

There are good reasons why we want to do this. Given the potential for benefit, it would be immoral to not attempt to use associated advances in technology to reduce humanities ailments and expand our condition.

There are also other issues competing for our concern. For example, it is probably inevitable that some potentially severe degree of civilisational disruption will occur due to anthropogenic climate change. Beyond that, there’s always the eventual heat death of the universe to plan for.

Whether or not we should be concerned about any of these issues and to what degree, is heavily influenced by our expectations with respect to timeframes. I believe that widely varying timeframe expectations explains much of the difference between the levels of concern about super-intelligence displayed by AI researchers and the wider public. I further contend that AI researchers respect the immense complexity of the task and the amount of work remaining and that this leads to their generally more conservative expectations.

### The Hype

The current media hype cycle is just about at frenzy point.

Many articles present the incremental progress and any new applications of modern machine learning techniques as rapid steps towards a fuzzy and general “AI” concept. AI super-intelligence is presented as a nearby, looming threat. More factually, there are indeed exciting achievements being announced, but they’re within much more narrow specifications than popular science and futurist publications often imply. SOURCES?

USE SOURCE EXAMPLES AND LINK TO DNN

These examples of hype revolve around the use of specific real technologies that we can investigate. It’s fun and interesting to try and conceptualise about the possibilities of future tech, like these articles do, but there are also many traps. It’s easy to over-generalise, to extrapolate far beyond the realm of the subject data and end up with conclusions that aren’t supported by evidence.

In the case of the articles I’ve highlighted here, the driving force behind the specific applications and advances is deep neural networks (DNN). DNNs were SOURCE HISTORY. People like to talk in loose ways about how neural networks are brain inspired systems. I’d like to point out here how new applications of this tool, combined with the evocative sense of that loose inspiration, is not sufficient justify any hype with respect to substantial progress towards “general” intelligence from which uncontrollable emergent consciousness (for example) might arise.

Using DNNs as an example, they are, I argue:
1. Not very deep. A depth of 30 nodes (a settable “hyperparameter” chosen by the person creating the network, with upside limited by hardware) is currently impressively deep (http://thenewstack.io/deep-learning-neural-networks-google-deep-dream/) whereas real neurons equivalent “depth” is between 2 and 10,000-100,000 (https://www.reddit.com/r/MachineLearning/comments/432hht/depth_of_the_human_neural_network/czfbj4n/).
2. Not really neural. Apart from being conceived by someone thinking about neurons, a node in a neural network is a simple number/transformation compared with the deeply complex neurons (of which there are many different types) in our brains. SOURCES
3. Barely networked. The number of local connections each neuron has in something like a convolutional neural network is also a hyperparameter but essentially always minuscule by comparison to their real squishy counterparts (http://www.human-memory.net/brain_neurons.html). SOURCES
IMPROVE SOURCES?

Fundamentally, deep neural nets don’t really model the brain and aren’t really particularly related to concepts of general intelligence (in my opinion) in the same way the brain might be (http://www.turingfinance.com/misconceptions-about-neural-networks/). This isn’t a criticism, proponents of deep neural nets might argue they’re not _trying_ to model the brain, but rather that they’re a very effective tool for detecting patterns in huge datasets within strict conditions.

It would be fallacious of me to suggest that there is literally no progress towards what might one day be general AI as a result of research into DNNs. I merely suggest that the progress is far less direct and substantial than is implied by articles in popular media.

### Reality

TODO: research is slow

### So, do we relax?

A group (LINK) associated with a podcast (LINK) I enjoy listening to recently asked for listener feedback about when we think the general AI that will [CHOOSE ONE] ( save the world | destroy us all) will drop. The format for answers is three numbers, in years from now, encompassing a range within which one believes this event will occur (with 90% confidence) and a point estimate within that.

Group members provided all kinds of answers, including estimates that are terrifyingly close and even long past (from a self professed simulist (LINK)








My point is that the hype that creates a sense of nearness with respect to a general form of AI does not match up with reality. AI research has progressed, like many fields, at a fairly steady pace since it’s inception.

Research is a slow process and we are so incredibly far from understanding the basis of intelligence itself, I think it is hopelessly optimistic to suggest we are anywhere near creating (and controlling) it artificially. (A cool read on that https://www.engadget.com/2016/08/15/technological-singularity-problems-brain-mind/). Ask a room full of neuroscientists and neurologists how intelligence works and you’ll get a bunch of laughter.





So, I’m not sure I agree that it’s meaningful to realistically describe progress towards general intelligence as a number.

I come originally from a finance background so I’m familiar with the power of exponential growth. However, for the concept of exponential growth to apply, certain conditions must be met. Growth must occur across the corpus of the subject at a stable rate over time, in order for growth to build on growth.

In finance, this is simple. You have capital k, and interest rate r. Capital is just a number of dollars. If it goes up from 100 to 105, then the next 5% increase will obviously take it to 110.25.

In our case, we’re talking about “growth of knowledge/capability in the field of artificial intelligence”, which is much broader and more complex than a number of dollars.

It’s not wholly clear to me which parts of that growth, or progress, will build on previous knowledge *in a way that is multiplicative with past or future progress*.

For example, if you learn something about intelligence, say that a goal setting mechanism can be influenced in a particular way, does that necessarily mean you will learn the next thing about intelligence faster in your research? Probably not. However some progress, for example increases in computer hardware speeds on which we run our AI software, definitely has increased along an exponential curve.

This actually provides a good example of what I’m talking about though.

Moore’s law predicted that the number of transistors on a circuitboard would double every two years. This law is often generalised in popular media along the lines of “computers get twice as fast every two years” or something similar. Now computers certainly did get much faster for a long time in this sort of way. However, even in this area growth is not simplistic enough to apply the concept of exponentially without some caveats. Due to real world physical limits and the properties of atomic particles, we can’t actually just continue to make things smaller forever. Various clever techniques continued progress through the 2000s for some time, but moore’s law has most definitely died (https://arstechnica.com/information-technology/2016/02/moores-law-really-is-dead-this-time/).

In reality, the concept of there being a “law” that enabled continuous exponential growth, was really just a nice narrative and in fact the concept of exponential growth in technological improvement is a huge oversimplification of the hugely multifarious nature of the growth itself, which is in fact many small unrelated discoveries, inventions, progresses made towards individual problems and so on.

Certainly, improvements in one area help improvements in another. The internet for example has massively increased the rate of growth in many other discovery pursuits. The development of GAI would most certainly do something similar, maybe even much more profoundly. However, we can’t simply generalise this concept to the field of AI research and just expect that all progress towards unlocking all the many capabilities required to actually create a GAI can be represented as a simple number and that all progress builds on itself in a multiplicative fashion and therefore will grow exponentially.

So to summarise another embarrassingly long Facebook comment, I think that it doesn’t necessarily make sense to consider “progress in science towards the ability to build general AI” as something simple enough to apply a linear or exponential growth model to. Progress in the field is much more nuanced, and while definitely increasing with increased resourcing in recent times, there is still a lot of slow, hard research to do in order to solve all the technical problems ahead.




Finally, just to be clear, I still think that the possibility of creating general artificial intelligence in 100 years is a terrifying concept and we’re being typically shortsighted as a species and not dealing with it properly at all. Hence my membership of this group and enjoyment of your podcasts!

It’s also true that, even if you can’t just use a number to describe progress and apply the concept of exponentially without some critical thinking, progress still builds on itself and accelerates. Things will take us by surprise




more notes 3:
I don't disagree that in progress will generally accelerate compared with current rates as a result of technological progress; perhaps just about how much, and how much work there is remaining.

For example if there were a linear increase in the rate of progress of n and an exponential increase of y, but y was very small it still might take many decades.

An annual 1% increase in the rate of progress still only results in progress at a rate of ~270% (compared with a present relative baseline of 100%) after 100 years.

If you were only progressing towards the eventual goal at a present rate of 0.6% per year of the amount of work required to achieve the goal, you would achieve your goal in 100 years, even with that exponential rate increase.

If we do abstract everything down to simple numbers, it's only which numbers we pick after all!

PS to be more specific and clear,

> the fact that lots of slow, incremental research needs to be done

just changes my expectation with respect to how much work is left, and to what extent it can and will be accelerated (and to what extent progress resultant from it's incremental achievement will contribute to exponential or linear (or no!) increase in the rate of progress itself)


NOTES BELOW HERE
## Is Superintelligence Coming?

notes:
Disclaimer: my opinions are based on relatively modest research as I’m pretty early in the journey. I invite any corrections to anything I’ve written that is wrong.

The current hype cycle presents our incremental improvements and applications of modern machine learning techniques as rapid steps towards a fuzzy and general “AI” concept. There are some cool sounding achievements being announced almost weekly, but they’re all still within extremely narrow specifications. 

Stepping back a bit, in my opinion most of modern machine learning techniques (an example chosen because they’re popular in the media at the moment) are more about smart programmers applying clever statistical techniques than scientists creating a process that might actually result (by itself, or individually significantly contribute to) in emergent consciousness or a generalised intelligence.

As an example, deep neural networks are:
- not very deep. A depth of 30 nodes (a settable hyperparameter chosen by the person creating the network, realistically limited by hardware) is impressively deep (http://thenewstack.io/deep-learning-neural-networks-google-deep-dream/) whereas real neurons equivalent “depth” is between 2 and 10,000-100,000 (https://www.reddit.com/r/MachineLearning/comments/432hht/depth_of_the_human_neural_network/czfbj4n/).
- not really neural. Apart from being loosely inspired by neurons, a node in a neural network is a simple number/transformation compared with the deeply complex neurons (of which there are many different types) in our brains.
- barely networked - the number of local connections each neuron has in something like a convolutional neural network is also a hyperparameter but essentially always minuscule by comparison to their real squishy counterparts (http://www.human-memory.net/brain_neurons.html).

Fundamentally, deep neural nets don’t really model the brain and aren’t really particularly related to concepts of general intelligence (in my opinion) in the same way the brain might be (http://www.turingfinance.com/misconceptions-about-neural-networks/).

My point is that the hype that creates a sense of nearness does not match up with reality. AI research has progressed, like many fields, at a fairly steady pace since it’s inception.

Research is a slow process and we are so incredibly far from understanding the basis of intelligence itself, I think it is hopelessly optimistic to suggest we are anywhere near creating (and controlling) it artificially. (A cool read on that https://www.engadget.com/2016/08/15/technological-singularity-problems-brain-mind/). Ask a room full of neuroscientists and neurologists how intelligence works and you’ll get a bunch of laughter.

This is getting a tad long for a Facebook comment and the arguments are not properly sourced so I’ll write up a proper blog post at some point as this is an interesting topic. I’ll share it here when I get around to it!

Finally, just to be clear, I still think that the possibility of creating general artificial intelligence in 100 years is a terrifying concept and we’re being typically shortsighted as a species and not dealing with it properly at all. Hence my membership of this group and enjoyment of your podcasts!

notes 2:
So, I’m not sure I agree that it’s meaningful to realistically describe progress towards general intelligence as a number.

I come originally from a finance background so I’m familiar with the power of exponential growth. However, for the concept of exponential growth to apply, certain conditions must be met. Growth must occur across the corpus of the subject at a stable rate over time, in order for growth to build on growth.

In finance, this is simple. You have capital k, and interest rate r. Capital is just a number of dollars. If it goes up from 100 to 105, then the next 5% increase will obviously take it to 110.25.

In our case, we’re talking about “growth of knowledge/capability in the field of artificial intelligence”, which is much broader and more complex than a number of dollars.

It’s not wholly clear to me which parts of that growth, or progress, will build on previous knowledge *in a way that is multiplicative with past or future progress*.

For example, if you learn something about intelligence, say that a goal setting mechanism can be influenced in a particular way, does that necessarily mean you will learn the next thing about intelligence faster in your research? Probably not. However some progress, for example increases in computer hardware speeds on which we run our AI software, definitely has increased along an exponential curve.

This actually provides a good example of what I’m talking about though.

Moore’s law predicted that the number of transistors on a circuitboard would double every two years. This law is often generalised in popular media along the lines of “computers get twice as fast every two years” or something similar. Now computers certainly did get much faster for a long time in this sort of way. However, even in this area growth is not simplistic enough to apply the concept of exponentially without some caveats. Due to real world physical limits and the properties of atomic particles, we can’t actually just continue to make things smaller forever. Various clever techniques continued progress through the 2000s for some time, but moore’s law has most definitely died (https://arstechnica.com/information-technology/2016/02/moores-law-really-is-dead-this-time/).

In reality, the concept of there being a “law” that enabled continuous exponential growth, was really just a nice narrative and in fact the concept of exponential growth in technological improvement is a huge oversimplification of the hugely multifarious nature of the growth itself, which is in fact many small unrelated discoveries, inventions, progresses made towards individual problems and so on.

Certainly, improvements in one area help improvements in another. The internet for example has massively increased the rate of growth in many other discovery pursuits. The development of GAI would most certainly do something similar, maybe even much more profoundly. However, we can’t simply generalise this concept to the field of AI research and just expect that all progress towards unlocking all the many capabilities required to actually create a GAI can be represented as a simple number and that all progress builds on itself in a multiplicative fashion and therefore will grow exponentially.

So to summarise another embarrassingly long Facebook comment, I think that it doesn’t necessarily make sense to consider “progress in science towards the ability to build general AI” as something simple enough to apply a linear or exponential growth model to. Progress in the field is much more nuanced, and while definitely increasing with increased resourcing in recent times, there is still a lot of slow, hard research to do in order to solve all the technical problems ahead.


more notes 3:
I don't disagree that in progress will generally accelerate compared with current rates as a result of technological progress; perhaps just about how much, and how much work there is remaining.

For example if there were a linear increase in the rate of progress of n and an exponential increase of y, but y was very small it still might take many decades.

An annual 1% increase in the rate of progress still only results in progress at a rate of ~270% (compared with a present relative baseline of 100%) after 100 years.

If you were only progressing towards the eventual goal at a present rate of 0.6% per year of the amount of work required to achieve the goal, you would achieve your goal in 100 years, even with that exponential rate increase.

If we do abstract everything down to simple numbers, it's only which numbers we pick after all!

PS to be more specific and clear,

> the fact that lots of slow, incremental research needs to be done

just changes my expectation with respect to how much work is left, and to what extent it can and will be accelerated (and to what extent progress resultant from it's incremental achievement will contribute to exponential or linear (or no!) increase in the rate of progress itself)
